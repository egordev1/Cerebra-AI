import torch
import torch.nn as nn
import re
import random

class SynthesisL1(nn.Module):
    def __init__(self):
        super().__init__()
        self.model_id = "Synthesis-L1"
        self.version = "1.0.0"
        
        # –°–ª–æ–≤–∞—Ä—å
        self.vocab = {
            '<PAD>': 0, '<UNK>': 1, '–ø—Ä–∏–≤–µ—Ç': 2, '–ø–æ–∫–∞': 3, '–∫–∞–∫': 4, 
            '–¥–µ–ª–∞': 5, '—á—Ç–æ': 6, '—Ç—ã': 7, '–º–æ–¥–µ–ª—å': 8, '–æ–±—É—á–µ–Ω–∏–µ': 9,
            '—Ç–µ—Å—Ç': 10, '—Å–∏—Å—Ç–µ–º–∞': 11, '—Ä–∞–±–æ—Ç–∞': 12, '—Ö–æ—Ä–æ—à–æ': 13, '–ø–ª–æ—Ö–æ': 14
        }
        self.reverse_vocab = {v: k for k, v in self.vocab.items()}
        
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
        self.embedding = nn.Embedding(len(self.vocab), 128)
        self.lstm = nn.LSTM(128, 256, batch_first=True)
        self.fc = nn.Linear(256, 2)  # 2 –∫–ª–∞—Å—Å–∞: –≤–æ–ø—Ä–æ—Å –∏–ª–∏ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
        
        print(f"üéØ –°–æ–∑–¥–∞–Ω–∞ {self.model_id}")
    
    def text_to_tokens(self, text):
        words = re.findall(r'\b\w+\b', text.lower())
        tokens = [self.vocab.get(word, 1) for word in words]  # 1 = <UNK>
        if len(tokens) < 10:
            tokens += [0] * (10 - len(tokens))  # –¥–æ–ø–æ–ª–Ω—è–µ–º –¥–æ 10 —Ç–æ–∫–µ–Ω–æ–≤
        else:
            tokens = tokens[:10]
        return torch.tensor(tokens).unsqueeze(0)
    
    def forward(self, x):
        x = self.embedding(x)
        lstm_out, (hidden, _) = self.lstm(x)
        output = self.fc(hidden[-1])
        return output
    
    def process(self, text):
        responses = {
            '–ø—Ä–∏–≤–µ—Ç': '–ü—Ä–∏–≤–µ—Ç! –Ø Cerebra AI —Å –º–æ–¥–µ–ª—å—é Synthesis-L1.',
            '–∫–∞–∫ –¥–µ–ª–∞': '–í—Å—ë –æ—Ç–ª–∏—á–Ω–æ! –ì–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ.',
            '—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å': '–Ø –º–æ–≥—É –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –∏ –æ–±—É—á–∞—Ç—å—Å—è.',
            '–æ–±—É—á–µ–Ω–∏–µ': '–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ real_training() –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.',
            '–ø–æ–∫–∞': '–î–æ —Å–≤–∏–¥–∞–Ω–∏—è!'
        }
        
        text_lower = text.lower()
        for key, response in responses.items():
            if key in text_lower:
                return response
        
        return f"–ü–æ–ª—É—á–∏–ª: '{text}'. –û–±—É—á–∏ –º–µ–Ω—è –¥–ª—è –ª—É—á—à–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤!"
    
    def prepare_data(self):
        texts = [
            "–∫–∞–∫ –¥–µ–ª–∞", "—á—Ç–æ –Ω–æ–≤–æ–≥–æ", "–∫–æ—Ç–æ—Ä—ã–π —á–∞—Å", "–≥–¥–µ —Ç—ã", 
            "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä", "—Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ", "–æ—Ç–ª–∏—á–Ω–∞—è –ø–æ–≥–æ–¥–∞", "–≤—Å–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ",
            "–∫–∞–∫ —Ç–µ–±—è –∑–æ–≤—É—Ç", "—á—Ç–æ —ç—Ç–æ", "–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è", "–∫–æ–≥–¥–∞ –ø—Ä–∏–¥–µ—à—å",
            "—Ç–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã", "–≤—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "—Ö–æ—Ä–æ—à–∞—è —Ä–∞–±–æ—Ç–∞", "–æ—Ç–ª–∏—á–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç—Å—è"
        ]
        labels = [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1]  # 0-–≤–æ–ø—Ä–æ—Å, 1-—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
        return texts, labels
    
    def real_train(self, epochs=5):
        print(f"üéì –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ {self.model_id}...")
        
        texts, labels = self.prepare_data()
        print(f"üìä –ü—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(texts)}")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        inputs = []
        targets = []
        for text, label in zip(texts, labels):
            tokens = self.text_to_tokens(text)
            inputs.append(tokens)
            targets.append(torch.tensor(label))
        
        # –û–±—É—á–µ–Ω–∏–µ
        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()
        
        self.train()
        
        for epoch in range(epochs):
            total_loss = 0
            correct = 0
            
            for i in range(len(inputs)):
                optimizer.zero_grad()
                
                output = self(inputs[i])
                loss = criterion(output, targets[i].unsqueeze(0))
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                
                # –°—á–∏—Ç–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
                pred = torch.argmax(output)
                if pred == targets[i]:
                    correct += 1
            
            accuracy = 100 * correct / len(inputs)
            avg_loss = total_loss / len(inputs)
            print(f"   –≠–ø–æ—Ö–∞ {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.1f}%")
        
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        return True
    
    def get_info(self):
        return {
            'model_id': self.model_id,
            'version': self.version,
            'parameters': sum(p.numel() for p in self.parameters())
        }