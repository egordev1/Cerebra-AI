import torch
import torch.nn as nn
import re
import random
import logging
import os

# –ò–º–ø–æ—Ä—Ç logger —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ—à–∏–±–æ–∫
try:
    from cerebra.logger_config import logger
except ImportError:
    logger = logging.getLogger('cerebra')
    if not logger.handlers:
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter('%(levelname)s - %(message)s'))
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)

# –ò–º–ø–æ—Ä—Ç GPT –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
try:
    from .gpt_model import GPTTransformer
    from .tokenizer import SimpleTokenizer
    from .training import prepare_training_data, train_gpt_model
    GPT_AVAILABLE = True
except ImportError:
    try:
        from cerebra.models.gpt_model import GPTTransformer
        from cerebra.models.tokenizer import SimpleTokenizer
        from cerebra.models.training import prepare_training_data, train_gpt_model
        GPT_AVAILABLE = True
    except ImportError:
        GPT_AVAILABLE = False
        logger.warning("GPT –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞—Ä–∞—è LSTM –º–æ–¥–µ–ª—å")

# –ò–º–ø–æ—Ä—Ç —Å–±–æ—Ä—â–∏–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤
try:
    from ..dialogue_training import dialogue_collector
except ImportError:
    try:
        from cerebra.dialogue_training import dialogue_collector
    except ImportError:
        dialogue_collector = None

class SynthesisL1(nn.Module):
    """
    –ì–ª–∞–≤–Ω–∞—è –º–æ–¥–µ–ª—å Synthesis-L1 - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è GPT —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∞—è –º–æ–¥–µ–ª—å
    """
    def __init__(self, use_gpt=True):
        super().__init__()
        self.model_id = "Synthesis-L1"
        self.version = "2.0.0" if use_gpt and GPT_AVAILABLE else "1.0.0"
        self.use_gpt = use_gpt and GPT_AVAILABLE
        
        if self.use_gpt:
            # GPT Transformer –º–æ–¥–µ–ª—å
            self.tokenizer = SimpleTokenizer(vocab_size=10000)
            self.gpt_model = GPTTransformer(
                vocab_size=10000,
                d_model=512,
                n_heads=8,
                n_layers=6,
                d_ff=2048,
                max_seq_len=512,
                dropout=0.1
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –µ—Å–ª–∏ –µ—Å—Ç—å
            tokenizer_path = "models/tokenizer.json"
            if os.path.exists(tokenizer_path):
                try:
                    self.tokenizer.load(tokenizer_path)
                    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å {self.tokenizer.get_vocab_size()} —Ç–æ–∫–µ–Ω–∞–º–∏")
                except:
                    logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π")
        else:
            # –°—Ç–∞—Ä–∞—è LSTM –º–æ–¥–µ–ª—å (fallback)
            self.vocab = {
                '<PAD>': 0, '<UNK>': 1, '–ø—Ä–∏–≤–µ—Ç': 2, '–ø–æ–∫–∞': 3, '–∫–∞–∫': 4, 
                '–¥–µ–ª–∞': 5, '—á—Ç–æ': 6, '—Ç—ã': 7, '–º–æ–¥–µ–ª—å': 8, '–æ–±—É—á–µ–Ω–∏–µ': 9,
                '—Ç–µ—Å—Ç': 10, '—Å–∏—Å—Ç–µ–º–∞': 11, '—Ä–∞–±–æ—Ç–∞': 12, '—Ö–æ—Ä–æ—à–æ': 13, '–ø–ª–æ—Ö–æ': 14
            }
            self.reverse_vocab = {v: k for k, v in self.vocab.items()}
            
            self.embedding = nn.Embedding(len(self.vocab), 128)
            self.lstm = nn.LSTM(128, 256, batch_first=True)
            self.fc = nn.Linear(256, 2)
        
        # –°–±–æ—Ä—â–∏–∫ –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        self.dialogue_collector = dialogue_collector
        
        # –§–ª–∞–≥ –æ–±—É—á–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
        self.is_trained = False
        if self.use_gpt:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
            model_path = "models/synthesis_l1_trained.pth"
            if os.path.exists(model_path):
                try:
                    checkpoint = torch.load(model_path, map_location='cpu')
                    if 'gpt_model_state_dict' in checkpoint:
                        self.gpt_model.load_state_dict(checkpoint['gpt_model_state_dict'])
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ –æ–±—É—á–µ–Ω–Ω–æ—Å—Ç–∏ –∏–∑ checkpoint
                        self.is_trained = checkpoint.get('is_trained', True)
                        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ GPT –º–æ–¥–µ–ª—å (–æ–±—É—á–µ–Ω–∞: {self.is_trained})")
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e}")
                    pass
        
        model_type = "GPT Transformer" if self.use_gpt else "LSTM"
        logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–∞ {self.model_id} v{self.version} ({model_type})")
        print(f"üéØ –°–æ–∑–¥–∞–Ω–∞ {self.model_id} v{self.version} ({model_type})")
        if self.use_gpt and not self.is_trained:
            print("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—É–Ω–∫—Ç 2 –≤ –º–µ–Ω—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.")
    
    def text_to_tokens(self, text, device=None):
        words = re.findall(r'\b\w+\b', text.lower())
        tokens = [self.vocab.get(word, 1) for word in words]  # 1 = <UNK>
        if len(tokens) < 10:
            tokens += [0] * (10 - len(tokens))  # –¥–æ–ø–æ–ª–Ω—è–µ–º –¥–æ 10 —Ç–æ–∫–µ–Ω–æ–≤
        else:
            tokens = tokens[:10]
        tensor = torch.tensor(tokens).unsqueeze(0)
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ç–µ–Ω–∑–æ—Ä –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        if device is not None:
            tensor = tensor.to(device)
        return tensor
    
    def forward(self, x):
        """Forward pass (—Ç–æ–ª—å–∫–æ –¥–ª—è LSTM –º–æ–¥–µ–ª–∏)"""
        if self.use_gpt:
            # –î–ª—è GPT –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ generate()
            raise NotImplementedError("–î–ª—è GPT –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ—Ç–æ–¥ generate() —á–µ—Ä–µ–∑ process()")
        x = self.embedding(x)
        lstm_out, (hidden, _) = self.lstm(x)
        output = self.fc(hidden[-1])
        return output
    
    def process(self, text):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º GPT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞
        if self.use_gpt and self.is_trained:
            try:
                device = next(self.gpt_model.parameters()).device
                self.gpt_model.eval()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
                vocab_size = self.tokenizer.get_vocab_size()
                if vocab_size <= len(self.tokenizer.special_tokens):
                    logger.warning("–°–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                    raise ValueError("–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –≥–æ—Ç–æ–≤")
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
                try:
                    generated = self.gpt_model.generate(
                        self.tokenizer,
                        prompt=text,
                        max_length=50,
                        temperature=0.8,
                        top_k=40,
                        top_p=0.9
                    )
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å (–±–µ–∑ –ø—Ä–æ–º–ø—Ç–∞)
                    prompt_lower = text.lower().strip()
                    generated_lower = generated.lower().strip()
                    
                    if generated_lower.startswith(prompt_lower):
                        generated = generated[len(prompt_lower):].strip()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ —Å–æ—Å—Ç–æ–∏—Ç —Ç–æ–ª—å–∫–æ –∏–∑ UNK –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
                    if not generated or len(generated) < 3:
                        raise ValueError("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—É—Å—Ç–∞")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ç–æ–ª—å–∫–æ UNK
                    unk_count = generated.lower().count('<unk>')
                    if unk_count > len(generated) * 0.5:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 50% UNK
                        logger.warning(f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ UNK –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ({unk_count}/{len(generated)})")
                        raise ValueError("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ UNK")
                    
                    logger.debug(f"GPT —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç: '{generated[:50]}...'")
                    return generated
                    
                except Exception as gen_error:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ GPT: {gen_error}")
                    raise  # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –¥–∞–ª—å—à–µ –¥–ª—è fallback
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ GPT: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
        elif self.use_gpt and not self.is_trained:
            # –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, —Å—Ä–∞–∑—É –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
            logger.debug("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –æ—Ç–≤–µ—Ç—ã")
        
        # Fallback: –±–∞–∑–æ–≤—ã–µ –æ—Ç–≤–µ—Ç—ã
        responses = {
            '–ø—Ä–∏–≤–µ—Ç': '–ü—Ä–∏–≤–µ—Ç! –Ø Cerebra AI —Å –º–æ–¥–µ–ª—å—é Synthesis-L1. –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?',
            '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π': '–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π! –ì–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ.',
            '–∫–∞–∫ –¥–µ–ª–∞': '–í—Å—ë –æ—Ç–ª–∏—á–Ω–æ! –ì–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ. –ê —É –≤–∞—Å –∫–∞–∫ –¥–µ–ª–∞?',
            '—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å': '–Ø –º–æ–≥—É –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—â–∞—Ç—å—Å—è.',
            '–ø–æ–∫–∞': '–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! –£–¥–∞—á–∏!',
        }
        
        text_lower = text.lower().strip()
        for key, response in responses.items():
            if key in text_lower:
                return response
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
        if self.use_gpt:
            return "–Ø –µ—â–µ –Ω–µ –æ–±—É—á–µ–Ω –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ö–æ—Ä–æ—à–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±—É—á–∏—Ç—å –º–µ–Ω—è —á–µ—Ä–µ–∑ –º–µ–Ω—é (–ø—É–Ω–∫—Ç 2)!"
        return f"–ü–æ–ª—É—á–∏–ª: '{text}'. –û–±—É—á–∏ –º–µ–Ω—è –¥–ª—è –ª—É—á—à–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤!"
    
    def prepare_data(self):
        texts = [
            "–∫–∞–∫ –¥–µ–ª–∞", "—á—Ç–æ –Ω–æ–≤–æ–≥–æ", "–∫–æ—Ç–æ—Ä—ã–π —á–∞—Å", "–≥–¥–µ —Ç—ã", 
            "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä", "—Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ", "–æ—Ç–ª–∏—á–Ω–∞—è –ø–æ–≥–æ–¥–∞", "–≤—Å–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ",
            "–∫–∞–∫ —Ç–µ–±—è –∑–æ–≤—É—Ç", "—á—Ç–æ —ç—Ç–æ", "–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è", "–∫–æ–≥–¥–∞ –ø—Ä–∏–¥–µ—à—å",
            "—Ç–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã", "–≤—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "—Ö–æ—Ä–æ—à–∞—è —Ä–∞–±–æ—Ç–∞", "–æ—Ç–ª–∏—á–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç—Å—è"
        ]
        labels = [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1]  # 0-–≤–æ–ø—Ä–æ—Å, 1-—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
        return texts, labels
    
    def real_train(self, epochs=10, batch_size=4, lr=3e-4):
        device = next(self.parameters()).device  # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
        
        if self.use_gpt:
            # GPT –æ–±—É—á–µ–Ω–∏–µ
            logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞—é GPT –æ–±—É—á–µ–Ω–∏–µ {self.model_id} –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ {device}...")
            print(f"üöÄ –ù–∞—á–∏–Ω–∞—é GPT –æ–±—É—á–µ–Ω–∏–µ {self.model_id}...")
            print(f"   –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: Transformer (GPT-like)")
            print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in self.gpt_model.parameters()):,}")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            texts, vocab_size = prepare_training_data(self.tokenizer)
            logger.info(f"üìä –ü—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(texts)}")
            logger.info(f"üìö –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}")
            print(f"üìä –ü—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(texts)}")
            print(f"üìö –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}")
            
            # –û–±—É—á–µ–Ω–∏–µ GPT
            success = train_gpt_model(
                self.gpt_model,
                self.tokenizer,
                texts,
                epochs=epochs,
                batch_size=batch_size,
                lr=lr,
                device=device
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            os.makedirs("models", exist_ok=True)
            self.tokenizer.save("models/tokenizer.json")
            logger.info("–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
            
            # –ü–æ–º–µ—á–∞–µ–º –º–æ–¥–µ–ª—å –∫–∞–∫ –æ–±—É—á–µ–Ω–Ω—É—é
            if success:
                self.is_trained = True
                logger.info("–ú–æ–¥–µ–ª—å –ø–æ–º–µ—á–µ–Ω–∞ –∫–∞–∫ –æ–±—É—á–µ–Ω–Ω–∞—è")
            
            return success
        else:
            # –°—Ç–∞—Ä–æ–µ LSTM –æ–±—É—á–µ–Ω–∏–µ (fallback)
            logger.info(f"üéì –ù–∞—á–∏–Ω–∞—é LSTM –æ–±—É—á–µ–Ω–∏–µ {self.model_id} –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ {device}...")
            print(f"üéì –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ {self.model_id}...")
            
            texts, labels = self.prepare_data()
            logger.info(f"üìä –ü—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(texts)}")
            print(f"üìä –ü—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(texts)}")
            
            inputs = []
            targets = []
            for text, label in zip(texts, labels):
                tokens = self.text_to_tokens(text, device=device)
                inputs.append(tokens)
                targets.append(torch.tensor(label, device=device))
            
            optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
            criterion = nn.CrossEntropyLoss()
            self.train()
            
            for epoch in range(epochs):
                total_loss = 0
                correct = 0
                
                for i in range(len(inputs)):
                    optimizer.zero_grad()
                    output = self(inputs[i])
                    loss = criterion(output, targets[i].unsqueeze(0))
                    loss.backward()
                    optimizer.step()
                    
                    total_loss += loss.item()
                    pred = torch.argmax(output)
                    if pred == targets[i]:
                        correct += 1
                
            accuracy = 100 * correct / len(inputs)
            avg_loss = total_loss / len(inputs)
            log_msg = f"   –≠–ø–æ—Ö–∞ {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.1f}%"
            logger.info(log_msg)
            print(log_msg)
        
        # –ü–æ–º–µ—á–∞–µ–º –º–æ–¥–µ–ª—å –∫–∞–∫ –æ–±—É—á–µ–Ω–Ω—É—é
        if self.use_gpt:
            self.is_trained = True
            logger.info("–ú–æ–¥–µ–ª—å –ø–æ–º–µ—á–µ–Ω–∞ –∫–∞–∫ –æ–±—É—á–µ–Ω–Ω–∞—è")
        
        logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        return True
    
    def get_info(self):
        if self.use_gpt:
            info = self.gpt_model.get_info()
            info['tokenizer_size'] = self.tokenizer.get_vocab_size()
            return info
        else:
            return {
                'model_id': self.model_id,
                'version': self.version,
                'parameters': sum(p.numel() for p in self.parameters()),
                'architecture': 'LSTM'
            }